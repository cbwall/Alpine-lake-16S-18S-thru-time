---
title: "Lake Microbiomes Through Time"
author: "C Wall"
date: "5/17/2022"
output:
  html_document:
    code_folding: hide
    toc: yes
    toc_depth: 4
    toc_float: yes
editor_options: 
  chunk_output_type: inline
---

```{r global options, results="hide", warning=FALSE, message=FALSE}
if (!require('knitr')) install.packages('knitr'); library('knitr')
knitr::opts_chunk$set(warning=FALSE, message=FALSE, fig.align='center')

# load packages
if (!require("pacman")) install.packages("pacman") # for rapid install if not in library

# use pacman to load CRAN packages missing
pacman::p_load('knitr', 'tidyverse', 'knitr', 'magrittr', 'effects', 'devtools',
               'stringi', 'dplyr', "ggplot2", "gridExtra", "dada2", "phyloseq", "vegan",
               "cowplot", "decontam","BiocManager", "dada2")

devtools::install_github("benjjneb/dada2", ref="v1.20") # update to most recent dada2


#upload Bioconductor (now BiocManager or R v. > 3.5.0 ), can specify different version in last line
# if (!require("BiocManager", quietly = TRUE))
# install.packages("BiocManager")

#install specific BiocManager packages
# BiocManager::install(c( "Decipher", "phangorn", "phyloseq"), update = TRUE, ask = FALSE)
```

add in and edit metadata
```{r edit metadata}

# edit metadata
md<-read.csv("data/metadata/lake_eDNA.rev.csv")
md$year <- sapply(strsplit(md$sample_year_extract_ID, "_"), `[`, 1) # extract sample names to YEARS
md$sample.ID <- sapply(strsplit(md$sample_year_extract_ID, "_"), `[`, 2) # extract sample names to YEARS

# these are extraction controls
md$year[md$year=='sierra']<- NA
md$year[md$year=='colombia']<- NA

# make a column for location
md$location <- md$sample_type
md$location[md$location=='blank' | 
                 md$location=='pcr_pos' | 
                 md$location=='pcr_neg' | 
                 md$location=='ext.blank'] <- "NA"

# change sample type to reflect eDNA or controls
md$sample_type[md$sample_type=='colombia' | md$sample_type=='sierra']<- "eDNA"

# made a sample or control factor
md$sample_control<- md$sample_type
md$sample_control[md$sample_control=='eDNA']<- "samples"

# ID the - controls
md$sample_control[md$sample_control=='blank' |
                 md$sample_control=='pcr_neg' | 
                 md$sample_control=='ext.blank'] <- "neg.controls"

# ID the + controls
md$sample_control[md$sample_control=='pcr_pos'] <- "pos.controls"

#### add this SampleNames to the metadata file
S1<-sapply(strsplit(md$submission_sample_ID, "_"), `[`, 2)
S2<-sapply(strsplit(md$submission_sample_ID, "_"), `[`, 3)
S.name<-sampleNames<-paste(S1,S2)
sampleNames<-gsub(" ", "_", S.name) # remove space and add an underscore

md$sampleNames<-(sampleNames)

# rearrange
run.metaD<- md %>% 
  dplyr::select(submission_sample_ID, sample_year_extract_ID, gene, sampleNames,
                year, location, site, 
                sample_type, sample_number, sample_control)

make.fac<-c("year", "location", "site")
run.metaD[make.fac] <- lapply(run.metaD[make.fac], factor) # make all these factors

# export clean metadata
write.csv(run.metaD, "data/metadata/run.metaD.csv")
#write.csv(run.metaD[c(1:10),], "data/metadata/16Stest.run.metaD.csv")

############################################
############################################


```

filter the trimmed (cut-adapt processed) FASTQ files
```{r filter and trim}
# read in the names of the fastq files
# perform some string manipulation to get lists of the forward and reverse fastq in matched order:


# load in the cut-adapt samples in the "trimmed" folder
miseq_path<-"data/16S_Yos_data/trimmed" # CHANGE to the directory containing the fastq files after unzipping.
list.files(miseq_path)

## Filter and Trim
### remove low quality reads, trim to consistent length

# Sort ensures forward/reverse reads are in same order
fnFs <- sort(list.files(miseq_path, pattern="_R1_trimmed.fastq"))
fnRs <- sort(list.files(miseq_path, pattern="_R2_trimmed.fastq"))

##### ##### ##### ##### 
##### had issue here, the way #s reading in not in order with metadata sheet...
# Extract sample names, assuming filenames have format: SAMPLENAME_XXX.fastq
sampleNames.p2 <- sapply(strsplit(fnFs, "_"), `[`, 2) # extract sample names
sampleNames.p3 <- sapply(strsplit(fnFs, "_"), `[`, 3) # extract the run # sample
sampleNames<-paste(sampleNames.p2,sampleNames.p3) # compile
sampleNames<-gsub(" ", "_", sampleNames) # remove space and add an underscore
##### ##### ##### ##### ##### 


# Specify the full path to the fnFs and fnRs
fnFs <- file.path(miseq_path, fnFs)
fnRs <- file.path(miseq_path, fnRs)
fnFs[1:3]
```

Inspect quality plot scores. Can then truncate based on quality for reads. The truncating value does not have to be same for F and R.
```{r filter and trim}
# quality score plot for forward reads
plotQualityProfile(fnFs[c(1,10)])

# quality score plot for reverse reads
plotQualityProfile(fnRs[c(2,8)])
```


```{r export}
# We define the filenames for the filtered fastq.gz files:

# Trimming and filtering is performed on paired reads jointly, i.e. both reads must pass the filter for the pair to pass.

filt_path <- file.path(miseq_path, "filtered") # Place filtered files in filtered/ subdirectory
if(!file_test("-d", filt_path)) dir.create(filt_path)

filtFs <- file.path(filt_path, paste0(sampleNames, "_F_trimfilt.fastq.gz"))
filtRs <- file.path(filt_path, paste0(sampleNames, "_R_trimfilt.fastq.gz"))


#### --- if not useing cut-adapt, Figaro is a nice option 
# must use raw fastq files. Here, Figaro recommends a 192/158 trim for max retension (84%)
# will trim primer for forward (19) and reverse (20)
# Run Figaro to get estiamtes of what the truncLen should be (called Trim Position)
# removing the trimleft since primers removed, also Figaro can't run with trimmed data so forego.
#### ---


# We combine these trimming parameters with standard filtering parameters, the most important being the enforcement of a maximum of **2 expected errors per-read** 

out <- filterAndTrim(fnFs, filtFs, fnRs, filtRs, truncLen=c(215,200), #trimLeft=c(19,20),
                     maxN=0, maxEE=c(2,2), truncQ=2, rm.phix=TRUE,
                     compress=TRUE, multithread=TRUE) # On Windows set multithread=FALSE
head(out)

write.csv(out, file="output/out.trim.csv")
```

###Infer sequence variants
After filtering, use high-resolution DADA2 method to to infer amplicon sequence variants (ASVs) exactly, without imposing any arbitrary threshhold 

In order to verify that the error rates have been reasonably well-estimated, we inspect the fit between the observed error rates (black points) and the fitted error rates (black lines) in Figure 1. These figures show the frequencies of each type of transition as a function of the quality.

```{r error rates}
### estimate the error rates
errF <- learnErrors(filtFs, multithread=TRUE)
saveRDS(errF, file="output/errF.rds")

errR <- learnErrors(filtRs, multithread=TRUE)
saveRDS(errR, file="output/errR.rds")


# plot error rates
plotErrors(errF, nominalQ=TRUE)
plotErrors(errR, nominalQ=TRUE)
```

Dereplication combines all identical sequencing reads into into “unique sequences” with a corresponding “abundance”: the number of reads with that unique sequence. Dereplication substantially reduces computation time by eliminating redundant comparisons.
```{r, dereplicate}
### Derep
derepFs <- derepFastq(filtFs, verbose=TRUE)
saveRDS(derepFs, file="output/derepFs.rds")

derepRs <- derepFastq(filtRs, verbose=TRUE)
saveRDS(derepRs, file="output/derepRs.rds")

# Name the derep-class objects by the sample names
names(derepFs) <- sampleNames
names(derepRs) <- sampleNames
```

The DADA2 method relies on a parameterized model of substitution errors to distinguish sequencing errors from real biological variation
```{r infer variants}
#The DADA2 sequence inference method can run in two different modes:
#####
dadaFs <-dada(derepFs, err=errF, multithread=2)
saveRDS(dadaFs, file="output/dadaFs.rds")

dadaRs <-dada(derepRs, err=errF, multithread=2)
saveRDS(dadaRs,file="output/dadaRs.rds")

# inspect data
dadaFs[[1]]
```

Construct sequence table and remove chimeras
```{r sequence table}
# The DADA2 method produces a sequence table that is a higher-resolution analogue of the common “OTU table”, i.e. a sample by sequence feature table valued by the number of times each sequence was observed in each sample.

# get sequences
head(getSequences(dadaFs[[2]]))

# merge
mergers <- mergePairs(dadaFs, derepFs, dadaRs, derepRs)
seqtab <- makeSequenceTable(mergers[!grepl("Mock", names(mergers))])
table(nchar(getSequences(seqtab)))

### save data
saveRDS(mergers, file="output/mergers.rds")
saveRDS(seqtab, file="output/seqtab.rds")
```

Chimeras have not yet been removed. The error model in the sequence inference algorithm does not include a chimera component, and therefore we expect this sequence table to include many chimeric sequences. We now remove chimeric sequences by comparing each inferred sequence to the others in the table, and removing those that can be reproduced by stitching together two more abundant sequences.

```{r remove chimera}
# remove chimera
seqtab.nochim <-removeBimeraDenovo(seqtab, method="consensus", multithread=TRUE, verbose=TRUE)
saveRDS(seqtab.nochim,file="output/seqtab.nochim.rds")

# Set multithread=TRUE to use all cores
sum(seqtab.nochim)/sum(seqtab) # 99% of samples kept

getN <-function(x)sum(getUniques(x))
track <-cbind(out,sapply(dadaFs, getN), 
              sapply(dadaRs, getN), sapply(mergers, getN),rowSums(seqtab.nochim))
colnames(track) <-c("input", "filtered", "denoisedF", "denoisedR", "merged", "nonchim")
rownames(track) <- sampleNames
head(track)
```

Assign taxonomy using the SILVA database for 16S
```{r taxonomic assignment}
taxa <- assignTaxonomy(seqtab.nochim, "data/silva/silva_nr99_v138.1_train_set.fa.gz", multithread=TRUE)

# may need to set R environmnet memory load here: 
# https://stackoverflow.com/questions/51248293/error-vector-memory-exhausted-limit-reached-r-3-5-0-macos

# to add in Species for 16S
taxa <- addSpecies(taxa, "data/silva/silva_species_assignment_v138.1.fa.gz")

# inspect taxonomic assignment
taxa.print <- taxa # Removing sequence rownames for display only
rownames(taxa.print) <- NULL
head(taxa.print)

# let's save this as .RData since it is so time consuming!
saveRDS(taxa, file="output/taxaTable.rds")
```


```{r load back in data}
###### load back data if R collapsed
out<- readRDS("output/out.trim.rds")
errF<- readRDS("output/errF.rds")
errR<- readRDS("output/errR.rds")
derepFs<- readRDS("output/derepFs.rds")
derepRs<- readRDS("output/derepRs.rds")
dadaFs<- readRDS("output/dadaFs.rds")
dadaRs<- readRDS("output/dadaRs.rds")
mergers<- readRDS("output/mergers.rds")
seqtab<- readRDS("output/seqtab.rds")
seqtab.nochim<- readRDS("output/seqtab.nochim.rds")
######
```

Combine data into a phyloseq object.  
The package phyloseq organizes and synthesizes the different data types from a typical amplicon sequencing experiment into a single data object that can be easily manipulated
```{r load back in taxa table}
taxa<- readRDS("output/taxaTable.rds")

## sample data
# metadata is run.metaD
all(rownames(seqtab.nochim) %in% run.metaD$sampleNames)

rownames(run.metaD) <- run.metaD$sampleNames

ps <-phyloseq(otu_table(seqtab.nochim, taxa_are_rows=FALSE),
              sample_data(run.metaD), 
              tax_table(taxa))
              

# make a string of DNA names and add to phyloseq
dna <- Biostrings::DNAStringSet(taxa_names(ps))
names(dna) <- taxa_names(ps)
ps <- merge_phyloseq(ps, dna)
taxa_names(ps) <- paste0("ASV", seq(ntaxa(ps)))

# save and reload ps object
saveRDS(ps, file="output/ps.rds")
ps<- readRDS("output/ps.rds")

#11,812 taxa in 191 samples
```


Colombia data still here, need to remove it, as well as any taxa that is uncharacterized
```{r remove Colombia and uncharacterized taxa}
# Show available ranks in the dataset
rank_names(ps)

table(tax_table(ps)[, "Phylum"], exclude = NULL)
table(tax_table(ps)[, "Kingdom"], exclude = NULL)

# remove NAs in taxonomic table
ps <- subset_taxa(ps, !is.na(Phylum) & !Phylum %in% c("", "uncharacterized"))
ps <- subset_taxa(ps, !is.na(Phylum) & !Phylum %in% c("", "Chloroplast"))

#191 samples

#######
#remove Colombia samples
ps<- subset_samples(ps, !(location %in% "colombia")) # removes colombia "real" samples

# controls from Colombia to remove 
remove.Col.cont<- c("colombia_pcr_NEG_2", "colombia_pcr_POS_16S", "colombia_pcr_NEG_1")

ps.Yos<- subset_samples(ps, !(sample_year_extract_ID %in% remove.Col.cont)) 
# 106 samples retained, 8544 reads

#######
# revise run.metaD to remove those samples too
run.metaD<-run.metaD[(!run.metaD$location=="colombia"),]
run.metaD<-run.metaD[!(run.metaD$sample_year_extract_ID=="colombia_pcr_NEG_2" |
                       run.metaD$sample_year_extract_ID=="colombia_pcr_POS_16S" |
                       run.metaD$sample_year_extract_ID=="colombia_pcr_NEG_1"),]
Yos.metaD<-run.metaD

# re-examine table, NAs gone, and all Colombia gone
table(tax_table(ps.Yos)[, "Phylum"], exclude = NULL)
sample_data(ps.Yos)
######

# now PS object has 106 samples, all Sierra + controls
```

Make a prevalance column so we can see how common these ASVs are
```{r prevalence}
# Compute prevalence of each feature, store as data.frame
prevdf = apply(X = otu_table(ps.Yos),
               MARGIN = ifelse(taxa_are_rows(ps), yes = 1, no = 2),
               FUN = function(x){sum(x > 0)})

# Add taxonomy and total read counts to this data.frame
prevdf = data.frame(Prevalence = prevdf,
                    TotalAbundance = taxa_sums(ps.Yos),
                    tax_table(ps.Yos))

plyr::ddply(prevdf, "Phylum", function(df1){cbind(mean(df1$Prevalence),sum(df1$Prevalence))})

#########
```


ID contaminants using the negative controls. We need to find out which ASVs are contams and remove them. 
```{r ID contaminants}
## ID contaminants
# first let's prune those not in at least 1 sample
ps.Yos <- prune_taxa(taxa_sums(ps.Yos) > 1, ps.Yos)

# remove samples with < 100 reads
ps.Yos <- prune_samples(sample_sums(ps.Yos) > 100, ps.Yos)

ps.Yos
# now 6125 taxa remain, 100 samples (6 samples removed)

sample_data(ps.Yos)$is.neg <- sample_data(ps.Yos)$sample_control == "neg.controls" 
contamdf.prev <- isContaminant(ps.Yos, method="prevalence", neg="is.neg")

table(contamdf.prev$contaminant) # which are contaminants? 5, 6120 not
head(which(contamdf.prev$contaminant))
```

remove contaminants and then remove the negative controls all together
```{r remove contaminants}

### remove contaminants
ps.noncontam <- prune_taxa(!contamdf.prev$contaminant, ps.Yos)
ps.noncontam # 6120 remain in 100 samples

rich<-estimate_richness(ps.noncontam, split = TRUE, measures = NULL)
plot_richness(ps.noncontam, x="year", measures=c("Observed", "Shannon")) + theme_bw()

########### let's inspect
df.noncontam <- as.data.frame(sample_data(ps.noncontam))
df.noncontam$LibrarySize <- sample_sums(ps.noncontam) # this is the # of reads
df.noncontam <- df.noncontam[order(df.noncontam$LibrarySize),]
df.noncontam$Index <- seq(nrow(df.noncontam))
########### 

# library size / number of reads
df.noncontam$LibrarySize

#remove neg controls
ps.noncontam.negout<- subset_samples(ps.noncontam, !(sample_control %in% "neg.controls"))

# 4777 taxa in 99 samples (+ controls still in here -- can use to assess accuracy in mock)

```

```{r final PS}
#remove the positive controls
ps.samples<- subset_samples(ps.noncontam.negout, !(sample_control %in% "pos.controls"))
# 97 samples remain -- no controls, + or -, no Colombia

PS.fin<-ps.samples

########### let's inspect
df.fin <- as.data.frame(sample_data(PS.fin))
df.fin$LibrarySize <- sample_sums(PS.fin) # this is the # of reads
df.fin$Index <- seq(nrow(df.fin))
########### 

saveRDS(PS.fin, file="output/PS.fin.rds")
```


```{r rarify or not}
rarecurve(otu_table(PS.fin), step=50, cex=0.5, label=FALSE)

# remove samples with < 5000 reads
PS.fin.prune <- prune_samples(sample_sums(PS.fin) > 5000, PS.fin) #89 samples
rarecurve(otu_table(PS.fin.rar), step=50, cex=0.5, label=FALSE)

############# rarefy without replacement, @ 5000, then 8 samples removed, 1753 ASVs
ps.rare = rarefy_even_depth(PS.fin.prune, rngseed=1000, 
                             sample.size=0.9*min(sample_sums(PS.fin.prune)), replace=F)

sort(rowSums(otu_table(ps.rare))) # rarify at 5000 reads
saveRDS(ps.rare, file="output/ps.rare.rds")

plot_richness(ps.rare, x="year", measures=c("Observed", "Shannon")) + theme_bw()

NMDSord <- ordinate(ps.rare, "NMDS", "bray")
p1 = plot_ordination(ps.rare, NMDSord, type="samples", color="site", title="taxa")
p1  + stat_ellipse(type = "norm", linetype = 1) +
  theme_bw()
print(p1)

```

```{r}
# inspect # of reads
sort(rowSums(otu_table(PS.fin))) #reads
rich<-estimate_richness(PS.fin, split = TRUE, measures = NULL)

# plot it
plot_richness(PS.fin, x="year", measures=c("Observed", "Shannon"))


########### plot the inspected figures
# figure formatting conditions
Fig.formatting<-(theme_classic()) +
  theme(text=element_text(size=10),
        axis.line=element_blank(),
        legend.text.align = 0,
        legend.text=element_text(size=10),
        #legend.title = element_blank(),
        panel.border = element_rect(fill=NA, colour = "black", size=1),
        aspect.ratio=1, 
        axis.ticks.length=unit(0.25, "cm"),
        axis.text.y=element_text(
          margin=unit(c(0.5, 0.5, 0.5, 0.5), "cm"), colour="black", size=10), 
        axis.text.x=element_text(
          margin=unit(c(0.5, 0.5, 0.5, 0.5), "cm"), colour="black", size=8)) +
  theme(legend.key.size = unit(0.4, "cm")) +
  theme(aspect.ratio=1.3) +
  theme(panel.spacing=unit(c(0, 0, 0, 0), "cm"))

#####
plot.inspect.reads.type<-ggplot(data=df, aes(x=Index, y=LibrarySize, color=location)) + 
  geom_point()+
  scale_color_manual(name="Sample Type", values = c("darkgoldenrod1", "cornflowerblue", "gray70")) +
  xlab("Sample Index") + ylab("Library Size") +
  ylim(0, 40000) +
  Fig.formatting
plot.inspect.reads.type

```


