---
title: "Lake Microbiomes Through Time"
author: "C Wall"
date: "5/17/2022"
output:
  html_document:
    code_folding: hide
    toc: yes
    toc_depth: 4
    toc_float: yes
editor_options: 
  chunk_output_type: inline
---

```{r global options, results="hide", warning=FALSE, message=FALSE}
if (!require('knitr')) install.packages('knitr'); library('knitr')
knitr::opts_chunk$set(warning=FALSE, message=FALSE, fig.align='center')

# load packages
if (!require("pacman")) install.packages("pacman") # for rapid install if not in library

# use pacman to load CRAN packages missing
pacman::p_load('knitr', 'tidyverse', 'knitr', 'magrittr', 'effects', 'devtools',
               'stringi', 'dplyr', "ggplot2", "gridExtra", "dada2", "phyloseq", "vegan",
               "cowplot", "decontam","BiocManager", "dada2", "microbiome")

devtools::install_github("benjjneb/dada2", ref="v1.20") # update to most recent dada2


#upload Bioconductor (now BiocManager or R v. > 3.5.0 ), can specify different version in last line
# if (!require("BiocManager", quietly = TRUE))
# install.packages("BiocManager")

#install specific BiocManager packages
BiocManager::install(c( "Decipher", "phangorn", "phyloseq", "microbiome"), update = TRUE, ask = FALSE)
```

add in and edit metadata
```{r edit metadata}

# edit metadata
md<-read.csv("data/metadata/lake_eDNA.rev.csv")
md$year <- sapply(strsplit(md$sample_year_extract_ID, "_"), `[`, 1) # extract sample names to YEARS
md$sample.ID <- sapply(strsplit(md$sample_year_extract_ID, "_"), `[`, 2) # extract sample names to YEARS

# these are extraction controls
md$year[md$year=='sierra']<- NA
md$year[md$year=='colombia']<- NA

# make a column for location
md$location <- md$sample_type
md$location[md$location=='blank' | 
                 md$location=='pcr_pos' | 
                 md$location=='pcr_neg' | 
                 md$location=='ext.blank'] <- "NA"

# change sample type to reflect eDNA or controls
md$sample_type[md$sample_type=='colombia' | md$sample_type=='sierra']<- "eDNA"

# made a sample or control factor
md$sample_control<- md$sample_type
md$sample_control[md$sample_control=='eDNA']<- "samples"

# ID the - controls
md$sample_control[md$sample_control=='blank' |
                 md$sample_control=='pcr_neg' | 
                 md$sample_control=='ext.blank'] <- "neg.controls"

# ID the + controls
md$sample_control[md$sample_control=='pcr_pos'] <- "pos.controls"

#### add this SampleNames to the metadata file
S1<-sapply(strsplit(md$submission_sample_ID, "_"), `[`, 2)
S2<-sapply(strsplit(md$submission_sample_ID, "_"), `[`, 3)
S.name<-sampleNames<-paste(S1,S2)
sampleNames<-gsub(" ", "_", S.name) # remove space and add an underscore

md$sampleNames<-(sampleNames)

# rearrange
run.metaD<- md %>% 
  dplyr::select(submission_sample_ID, sample_year_extract_ID, gene, sampleNames,
                year, location, site, 
                sample_type, sample_number, sample_control)

make.fac<-c("year", "location", "site")
run.metaD[make.fac] <- lapply(run.metaD[make.fac], factor) # make all these factors

# export clean metadata
write.csv(run.metaD, "data/metadata/run.metaD.csv")
#write.csv(run.metaD[c(1:10),], "data/metadata/16Stest.run.metaD.csv")

############################################
############################################


```

filter the trimmed (cut-adapt processed) FASTQ files
```{r filter and trim}
# read in the names of the fastq files
# perform some string manipulation to get lists of the forward and reverse fastq in matched order:


# load in the cut-adapt samples in the "trimmed" folder
miseq_path<-"data/16S_Yos_data/trimmed" # CHANGE to the directory containing the fastq files after unzipping.
list.files(miseq_path)

## Filter and Trim
### remove low quality reads, trim to consistent length

# Sort ensures forward/reverse reads are in same order
# Sort ensures forward/reverse reads are in same order
fnFs <- sort(list.files(miseq_path, pattern="_R1_trimmed.fastq.gz"))
fnRs <- sort(list.files(miseq_path, pattern="_R2_trimmed.fastq.gz"))

##### ##### ##### ##### 
##### had issue here, the way #s reading in not in order with metadata sheet...
# Extract sample names, assuming filenames have format: SAMPLENAME_XXX.fastq
sampleNames.p2 <- sapply(strsplit(fnFs, "_"), `[`, 2) # extract sample names
sampleNames.p3 <- sapply(strsplit(fnFs, "_"), `[`, 3) # extract the run # sample
sampleNames<-paste(sampleNames.p2,sampleNames.p3) # compile
sampleNames<-gsub(" ", "_", sampleNames) # remove space and add an underscore

# merge these carefully
Sequencing_ID<-paste("CBW",sampleNames) # compile
submission_sample_ID<-gsub(" ", "_", Sequencing_ID) # remove space and add an underscore

SN.FQ<-as.data.frame(cbind(submission_sample_ID,sampleNames))

# if need to merge by a column, say if sequences not all in a single run or separated for some reason...
run.metaD.merge <- merge(run.metaD, SN.FQ, by="submission_sample_ID")
write.csv(run.metaD.merge, "output/16S_Yos_data/run.metaD.edit.csv")

# Specify the full path to the fnFs and fnRs
fnFs <- file.path(miseq_path, fnFs)
fnRs <- file.path(miseq_path, fnRs)
fnFs[1:3]
```

Inspect quality plot scores. Can then truncate based on quality for reads. The truncating value does not have to be same for F and R.
```{r filter and trim}
# quality score plot for forward reads
plotQualityProfile(fnFs[c(1,10)])

# quality score plot for reverse reads
plotQualityProfile(fnRs[c(2,8)])
```


```{r export}
# We define the filenames for the filtered fastq.gz files:

# Trimming and filtering is performed on paired reads jointly, i.e. both reads must pass the filter for the pair to pass.

filt_path <- file.path(miseq_path, "filtered") # Place filtered files in filtered/ subdirectory
if(!file_test("-d", filt_path)) dir.create(filt_path)

filtFs <- file.path(filt_path, paste0(sampleNames, "_F_trimfilt.fastq.gz"))
filtRs <- file.path(filt_path, paste0(sampleNames, "_R_trimfilt.fastq.gz"))


#### --- if not useing cut-adapt, Figaro is a nice option 
# must use raw fastq files. Here, Figaro recommends a 192/158 trim for max retension (84%)
# will trim primer for forward (19) and reverse (20)
# Run Figaro to get estiamtes of what the truncLen should be (called Trim Position)
# removing the trimleft since primers removed, also Figaro can't run with trimmed data so forego.
#### ---


# We combine these trimming parameters with standard filtering parameters, the most important being the enforcement of a maximum of **2 expected errors per-read** 

out <- filterAndTrim(fnFs, filtFs, fnRs, filtRs, truncLen=c(215,200), #trimLeft=c(19,20),
                     maxN=0, maxEE=c(2,2), truncQ=2, rm.phix=TRUE,
                     compress=TRUE, multithread=TRUE) # On Windows set multithread=FALSE
head(out)

write.csv(out, file="output/out.trim.csv")
```

###Infer sequence variants
After filtering, use high-resolution DADA2 method to to infer amplicon sequence variants (ASVs) exactly, without imposing any arbitrary threshhold 

In order to verify that the error rates have been reasonably well-estimated, we inspect the fit between the observed error rates (black points) and the fitted error rates (black lines) in Figure 1. These figures show the frequencies of each type of transition as a function of the quality.

```{r error rates}
### estimate the error rates
errF <- learnErrors(filtFs, multithread=TRUE)
saveRDS(errF, file="output/errF.rds")

errR <- learnErrors(filtRs, multithread=TRUE)
saveRDS(errR, file="output/errR.rds")


# plot error rates
plotErrors(errF, nominalQ=TRUE)
plotErrors(errR, nominalQ=TRUE)
```

Dereplication combines all identical sequencing reads into into “unique sequences” with a corresponding “abundance”: the number of reads with that unique sequence. Dereplication substantially reduces computation time by eliminating redundant comparisons.
```{r, dereplicate}
### Derep
derepFs <- derepFastq(filtFs, verbose=TRUE)
saveRDS(derepFs, file="output/derepFs.rds")

derepRs <- derepFastq(filtRs, verbose=TRUE)
saveRDS(derepRs, file="output/derepRs.rds")

# Name the derep-class objects by the sample names
names(derepFs) <- sampleNames
names(derepRs) <- sampleNames
```

The DADA2 method relies on a parameterized model of substitution errors to distinguish sequencing errors from real biological variation
```{r infer variants}
#The DADA2 sequence inference method can run in two different modes:
#####
dadaFs <-dada(derepFs, err=errF, multithread=2)
saveRDS(dadaFs, file="output/dadaFs.rds")

dadaRs <-dada(derepRs, err=errF, multithread=2)
saveRDS(dadaRs,file="output/dadaRs.rds")

# inspect data
dadaFs[[1]]
```

Construct sequence table and remove chimeras
```{r sequence table}
# The DADA2 method produces a sequence table that is a higher-resolution analogue of the common “OTU table”, i.e. a sample by sequence feature table valued by the number of times each sequence was observed in each sample.

# get sequences
head(getSequences(dadaFs[[2]]))

# merge
mergers <- mergePairs(dadaFs, derepFs, dadaRs, derepRs)
seqtab <- makeSequenceTable(mergers[!grepl("Mock", names(mergers))])
table(nchar(getSequences(seqtab)))

### save data
saveRDS(mergers, file="output/mergers.rds")
saveRDS(seqtab, file="output/seqtab.rds")
```

Chimeras have not yet been removed. The error model in the sequence inference algorithm does not include a chimera component, and therefore we expect this sequence table to include many chimeric sequences. We now remove chimeric sequences by comparing each inferred sequence to the others in the table, and removing those that can be reproduced by stitching together two more abundant sequences.

```{r remove chimera}
# remove chimera
seqtab.nochim <-removeBimeraDenovo(seqtab, method="consensus", multithread=TRUE, verbose=TRUE)
saveRDS(seqtab.nochim,file="output/seqtab.nochim.rds")

# Set multithread=TRUE to use all cores
sum(seqtab.nochim)/sum(seqtab) # 99% of samples kept

getN <-function(x)sum(getUniques(x))
track <-cbind(out,sapply(dadaFs, getN), 
              sapply(dadaRs, getN), sapply(mergers, getN),rowSums(seqtab.nochim))
colnames(track) <-c("input", "filtered", "denoisedF", "denoisedR", "merged", "nonchim")
rownames(track) <- sampleNames
head(track)
```

Assign taxonomy using the SILVA database for 16S
```{r taxonomic assignment}
taxa <- assignTaxonomy(seqtab.nochim, "data/silva/silva_nr99_v138.1_train_set.fa.gz", multithread=TRUE)

# may need to set R environmnet memory load here: 
# https://stackoverflow.com/questions/51248293/error-vector-memory-exhausted-limit-reached-r-3-5-0-macos

# to add in Species for 16S
taxa <- addSpecies(taxa, "data/silva/silva_species_assignment_v138.1.fa.gz")

# inspect taxonomic assignment
taxa.print <- taxa # Removing sequence rownames for display only
rownames(taxa.print) <- NULL
head(taxa.print)

# let's save this as .RData since it is so time consuming!
saveRDS(taxa, file="output/taxaTable.rds")
```


```{r load back in data}
###### load back data if R collapsed
out<- readRDS("output/out.trim.rds")
errF<- readRDS("output/errF.rds")
errR<- readRDS("output/errR.rds")
derepFs<- readRDS("output/derepFs.rds")
derepRs<- readRDS("output/derepRs.rds")
dadaFs<- readRDS("output/dadaFs.rds")
dadaRs<- readRDS("output/dadaRs.rds")
mergers<- readRDS("output/mergers.rds")
seqtab<- readRDS("output/seqtab.rds")
seqtab.nochim<- readRDS("output/seqtab.nochim.rds")
######
```

Combine data into a phyloseq object.  
The package phyloseq organizes and synthesizes the different data types from a typical amplicon sequencing experiment into a single data object that can be easily manipulated
```{r load back in taxa table}
taxa<- readRDS("output/taxaTable.rds")

## sample data
# metadata is run.metaD
all(rownames(seqtab.nochim) %in% run.metaD$sampleNames)

rownames(run.metaD) <- run.metaD$sampleNames

ps <-phyloseq(otu_table(seqtab.nochim, taxa_are_rows=FALSE),
              sample_data(run.metaD), 
              tax_table(taxa))
              

# make a string of DNA names and add to phyloseq
dna <- Biostrings::DNAStringSet(taxa_names(ps))
names(dna) <- taxa_names(ps)
ps <- merge_phyloseq(ps, dna)
taxa_names(ps) <- paste0("ASV", seq(ntaxa(ps)))

# save and reload ps object
saveRDS(ps, file="output/ps.rds")
ps<- readRDS("output/ps.rds")

#11,812 taxa in 191 samples
```


Colombia data still here, need to remove it, as well as any taxa that is uncharacterized
```{r remove Colombia and uncharacterized taxa}
# Show available ranks in the dataset
rank_names(ps)

table(tax_table(ps)[, "Phylum"], exclude = NULL)
table(tax_table(ps)[, "Kingdom"], exclude = NULL)

# remove NAs in taxonomic table
ps <- subset_taxa(ps, !is.na(Phylum) & !Phylum %in% c("", "uncharacterized"))
ps <- subset_taxa(ps, !is.na(Phylum) & !Phylum %in% c("", "Chloroplast"))

#191 samples

#######
#remove Colombia samples
ps<- subset_samples(ps, !(location %in% "colombia")) # removes colombia "real" samples

# controls from Colombia to remove 
remove.Col.cont<- c("colombia_pcr_NEG_2", "colombia_pcr_POS_16S", "colombia_pcr_NEG_1")

ps.Yos<- subset_samples(ps, !(sample_year_extract_ID %in% remove.Col.cont)) 
# 106 samples retained, 8544 reads

#######
# revise run.metaD to remove those samples too
run.metaD<-run.metaD[(!run.metaD$location=="colombia"),]
run.metaD<-run.metaD[!(run.metaD$sample_year_extract_ID=="colombia_pcr_NEG_3" |
                       run.metaD$sample_year_extract_ID=="colombia_pcr_POS_16S" |
                       run.metaD$sample_year_extract_ID=="colombia_pcr_NEG_4"),]
Yos.metaD<-run.metaD

# re-examine table, NAs gone, and all Colombia gone
table(tax_table(ps.Yos)[, "Phylum"], exclude = NULL)
sample_data(ps.Yos)
######

# now PS object has 106 samples, all Sierra + controls
```

Make a prevalance column so we can see how common these ASVs are
```{r prevalence}
# Compute prevalence of each feature, store as data.frame
prevdf = apply(X = otu_table(ps.Yos),
               MARGIN = ifelse(taxa_are_rows(ps), yes = 1, no = 2),
               FUN = function(x){sum(x > 0)})

# Add taxonomy and total read counts to this data.frame
prevdf = data.frame(Prevalence = prevdf,
                    TotalAbundance = taxa_sums(ps.Yos),
                    tax_table(ps.Yos))

plyr::ddply(prevdf, "Phylum", function(df1){cbind(mean(df1$Prevalence),sum(df1$Prevalence))})

#########
```


ID contaminants using the negative controls. We need to find out which ASVs are contams and remove them. 
```{r ID contaminants}
## ID contaminants
# first let's prune those not in at least 1 sample
ps.Yos <- prune_taxa(taxa_sums(ps.Yos) > 1, ps.Yos)

# remove samples with < 100 reads
ps.Yos <- prune_samples(sample_sums(ps.Yos) > 100, ps.Yos)

ps.Yos
# now 6125 taxa remain, 100 samples (6 samples removed)

sample_data(ps.Yos)$is.neg <- sample_data(ps.Yos)$sample_control == "neg.controls" 
contamdf.prev <- isContaminant(ps.Yos, method="prevalence", neg="is.neg")

table(contamdf.prev$contaminant) # which are contaminants? 5, 6120 not
head(which(contamdf.prev$contaminant))
```

remove contaminants and then remove the negative controls all together
```{r remove contaminants}

### remove contaminants
ps.noncontam <- prune_taxa(!contamdf.prev$contaminant, ps.Yos)
ps.noncontam # 6120 remain in 100 samples

rich<-estimate_richness(ps.noncontam, split = TRUE, measures = NULL)
plot_richness(ps.noncontam, x="year", measures=c("Observed", "Shannon")) + theme_bw()

########### let's inspect
df.noncontam <- as.data.frame(sample_data(ps.noncontam))
df.noncontam$LibrarySize <- sample_sums(ps.noncontam) # this is the # of reads
df.noncontam <- df.noncontam[order(df.noncontam$LibrarySize),]
df.noncontam$Index <- seq(nrow(df.noncontam))
########### 

# library size / number of reads
df.noncontam$LibrarySize

#remove neg controls
ps.noncontam.negout<- subset_samples(ps.noncontam, !(sample_control %in% "neg.controls"))

# 4777 taxa in 99 samples (+ controls still in here -- can use to assess accuracy in mock)

```

```{r final PS}
#remove the positive controls
ps.samples<- subset_samples(ps.noncontam.negout, !(sample_control %in% "pos.controls"))
# 97 samples remain -- no controls, + or -, no Colombia

PS.fin<-ps.samples

########### let's inspect
df.fin <- as.data.frame(sample_data(PS.fin))
df.fin$LibrarySize <- sample_sums(PS.fin) # this is the # of reads
df.fin$Index <- seq(nrow(df.fin))
########### 

saveRDS(PS.fin, file="output/PS.fin.rds")

# Make a data frame with a column for the read counts of each sample
sample_sum_df <- data.frame(sum = sample_sums(PS.fin))

# Histogram of sample read counts
hist.depth<-ggplot(sample_sum_df, aes(x = sum)) + 
  geom_histogram(color = "black", fill = "gold2", binwidth = 2500) +
  ggtitle("Distribution of sample sequencing depth") + 
  xlab("Read counts") +
  theme(axis.title.y = element_blank()) + theme_classic() + geom_vline(xintercept=5000, lty=2)
  

hist.depth
dev.copy(pdf, "figures/hist.depth.pdf", height=4, width=5)
dev.off() 
```


```{r rarify or not}

pdf(file="figures/rare.raw.pdf", height=4, width=5)
rarecurve(otu_table(PS.fin), step=50, cex=0.5, label=FALSE)
abline(v = 5000, lty = "dotted", col="red", lwd=2)
dev.off() 

# remove samples with < 5000 reads
PS.fin.prune <- prune_samples(sample_sums(PS.fin) > 5000, PS.fin) #89 samples
rarecurve(otu_table(PS.fin.rar), step=50, cex=0.5, label=FALSE)

############# rarefy without replacement, @ 5000, then 8 samples removed, 1753 ASVs
ps.rare = rarefy_even_depth(PS.fin.prune, rngseed=1000, 
                             sample.size=0.9*min(sample_sums(PS.fin.prune)), replace=F)

sort(rowSums(otu_table(ps.rare))) # rarify at 5000 reads
saveRDS(ps.rare, file="output/ps.rare.rds")
```

```{r save and export files}
#write phyloseq and end this first step
ps.rare.sample.df<-data.frame(sample_data(ps.rare))
ps.rare.ASV.df<-data.frame(otu_table(ps.rare))
ps.rare.tax.df<-data.frame(tax_table(ps.rare))
ps.rare.seq.df<-data.frame(refseq(ps.rare))

write.csv(ps.rare.sample.df, "output/phyloseq_elements/ps.rare.sample.df.csv")
write.csv(ps.rare.ASV.df, "output/phyloseq_elements/ps.rare.ASV.df.csv")
write.csv(ps.rare.tax.df, "output/phyloseq_elements/ps.rare.tax.df.csv")
write.csv(ps.rare.seq.df, "output/phyloseq_elements/ps.rare.seq.df.csv")
```

```{r ps melt}
### Fast Melt code function for quick taxa summary 

fast_melt = function(physeq,
                     includeSampleVars = character(),
                     omitZero = FALSE){
    require("phyloseq")
    require("data.table")
    # supports "naked" otu_table as `physeq` input.
    otutab = as(otu_table(physeq), "matrix")
    if(!taxa_are_rows(physeq)){otutab <- t(otutab)}
    otudt = data.table(otutab, keep.rownames = TRUE)
    setnames(otudt, "rn", "TaxaID")
    # Enforce character TaxaID key
    otudt[, TaxaIDchar := as.character(TaxaID)]
    otudt[, TaxaID := NULL]
    setnames(otudt, "TaxaIDchar", "TaxaID")
    # Melt count table
    mdt = melt.data.table(otudt, 
                          id.vars = "TaxaID",
                          variable.name = "SampleID",
                          value.name = "count")
    if(omitZero){
        # Omit zeroes and negative numbers
        mdt <- mdt[count > 0]
    }
    # Omit NAs
    mdt <- mdt[!is.na(count)]
    # Calculate relative abundance
    mdt[, RelativeAbundance := count / sum(count), by = SampleID]
    if(!is.null(tax_table(physeq, errorIfNULL = FALSE))){
        # If there is a tax_table, join with it. Otherwise, skip this join.
        taxdt = data.table(as(tax_table(physeq, errorIfNULL = TRUE), "matrix"), keep.rownames = TRUE)
        setnames(taxdt, "rn", "TaxaID")
        # Enforce character TaxaID key
        taxdt[, TaxaIDchar := as.character(TaxaID)]
        taxdt[, TaxaID := NULL]
        setnames(taxdt, "TaxaIDchar", "TaxaID")
        # Join with tax table
        setkey(taxdt, "TaxaID")
        setkey(mdt, "TaxaID")
        mdt <- taxdt[mdt]
    }
    # includeSampleVars = c("DaysSinceExperimentStart", "SampleType")
    # includeSampleVars = character()
    # includeSampleVars = c()
    # includeSampleVars = c("aksjdflkas") 
    wh.svars = which(sample_variables(physeq) %in% includeSampleVars)
    if( length(wh.svars) > 0 ){
        # Only attempt to include sample variables if there is at least one present in object
        sdf = as(sample_data(physeq), "data.frame")[, wh.svars, drop = FALSE]
        sdt = data.table(sdf, keep.rownames = TRUE)
        setnames(sdt, "rn", "SampleID")
        # Join with long table
        setkey(sdt, "SampleID")
        setkey(mdt, "SampleID")
        mdt <- sdt[mdt]
    }
    setkey(mdt, "TaxaID")
    return(mdt)
}

summarize_taxa = function(physeq, Rank, GroupBy = NULL){
    require("phyloseq")
    require("data.table")
    Rank <- Rank[1]
    if(!Rank %in% rank_names(physeq)){
        message("The argument to `Rank` was:\n", Rank,
                "\nBut it was not found among taxonomic ranks:\n",
                paste0(rank_names(physeq), collapse = ", "), "\n",
                "Please check the list shown above and try again.")
    }
    if(!is.null(GroupBy)){
        GroupBy <- GroupBy[1]
        if(!GroupBy %in% sample_variables(physeq)){
            message("The argument to `GroupBy` was:\n", GroupBy,
                    "\nBut it was not found among sample variables:\n",
                    paste0(sample_variables(physeq), collapse = ", "), "\n",
                    "Please check the list shown above and try again.")
        }
    }
    # Start with fast melt
    mdt = fast_melt(physeq)
    if(!is.null(GroupBy)){
        # Add the variable indicated in `GroupBy`, if provided.
        sdt = data.table(SampleID = sample_names(physeq),
                         var1 = get_variable(physeq, GroupBy))
        setnames(sdt, "var1", GroupBy)
        # Join
        setkey(sdt, SampleID)
        setkey(mdt, SampleID)
        mdt <- sdt[mdt]
    }
    # Summarize
    if(!is.null(GroupBy)){
        summarydt = mdt[, list(meanRA = mean(RelativeAbundance),
                               sdRA = sd(RelativeAbundance),
                               minRA = min(RelativeAbundance),
                               maxRA = max(RelativeAbundance)),
                        by = c(Rank, GroupBy)]
    } else {
        Nsamples = nsamples(physeq)
        # No GroupBy argument, can be more precise with the mean, sd, etc.
        summarydt = mdt[, list(meanRA = sum(RelativeAbundance) / Nsamples,
                               sdRA = sd(c(RelativeAbundance, numeric(Nsamples - .N))),
                               minRA = ifelse(test = .N < Nsamples,
                                              yes = 0L, 
                                              no = min(RelativeAbundance)),
                               maxRA = max(RelativeAbundance)),
                        by = c(Rank)]
    }
    return(summarydt)
}

plot_taxa_summary = function(physeq, Rank, GroupBy = NULL){
    require("phyloseq")
    require("data.table")
    require("ggplot2")
    # Get taxa summary table 
    dt1 = summarize_taxa(physeq, Rank = Rank, GroupBy = GroupBy)
    # Set factor appropriately for plotting
    RankCol = which(colnames(dt1) == Rank)
    setorder(dt1, -meanRA)
    dt1[, RankFac := factor(dt1[[Rank]], 
                            levels = rev(dt1[[Rank]]))]
    dt1[, ebarMax := max(c(0, min(meanRA + sdRA))), by = eval(Rank)]
    dt1[, ebarMin := max(c(0, min(meanRA - sdRA))), by = eval(Rank)]
    # Set zeroes to one-tenth the smallest value
    ebarMinFloor = dt1[(ebarMin > 0), min(ebarMin)]
    ebarMinFloor <- ebarMinFloor / 10
    dt1[(ebarMin == 0), ebarMin := ebarMinFloor]
    
    pRank = ggplot(dt1, aes(x = meanRA, y = RankFac)) +
        scale_x_log10() +
        xlab("Mean Relative Abundance") +
        ylab(Rank) +
        theme_bw()
    if(!is.null(GroupBy)){
        # pRank <- pRank + facet_wrap(facets = as.formula(paste("~", GroupBy)))
        pRank <- pRank + geom_point(mapping = aes_string(colour = GroupBy),
                                    size = 5)
    } else {
        # Don't include error bars for faceted version
        pRank <- pRank + geom_errorbarh(aes(xmax = ebarMax,
                                            xmin = ebarMin))
    }
    return(pRank)
}
```


```{r PERMANOVA}

### Phyloseq ###
########################## NMDS
# the saved phyloseq ASV table is 'ps.rare.ASV.df'

# Bray Curtis distance matrix
bc_dist = as.matrix((vegdist(ps.rare.ASV.df, "bray")))

# Bray PERMANOVA 
bc.adonis <- adonis2(bc_dist~site*year, data=ps.rare.sample.df, permutations = 9999)
bc.adonis # site, year effect

dd<-vegdist(bc_dist, method="bray")

######## test for beta dispersion by YEAR
mod.beta.year<-betadisper(dd, ps.rare.sample.df$year) # multivariate dispersions
anova(mod.beta.year) # not significant by YEAR  (equal dispersion)
# cannot reject null hypothesis that the groups have equal dispersion

plot(mod.beta.year)
boxplot(mod.beta.year)

### test for beta dispersion by SITE
mod.beta.site<-betadisper(dd, ps.rare.sample.df$site) # multivariate dispersions
anova(mod.beta.year) # significant by site  (unequal dispersion)
#  reject null hypothesis that the groups have equal dispersion

plot(mod.beta.site)
boxplot(mod.beta.site)
```


```{r plots and analysis, eval =FALSE}
# base code approach to NMDS

# k = 3 dimensions
set.seed(520)
NMDS = metaMDS(bc_dist, k=3, trymax=100)
NMDS1=NMDS$points[,1]
NMDS2=NMDS$points[,2]
NMDS.plot.df=data.frame(NMDS1=NMDS1,NMDS2=NMDS2, 
                        site=ps.rare.sample.df$site,
                        year=ps.rare.sample.df$year)


#####################
# plotting elements
year.group<-NMDS.plot.df$year
site.group<-NMDS.plot.df$site
# make colors for year
year.col<-c("coral","mediumseagreen","goldenrod","dodgerblue")

# make colors for sites
library(RColorBrewer)
qual_col_pals = brewer.pal.info[brewer.pal.info$category == 'qual',]
col_vector = unlist(mapply(brewer.pal, qual_col_pals$maxcolors, rownames(qual_col_pals)))

########################

### make plot by habitat with vectors for environment
ordiplot(NMDS, type="n", main=substitute(paste("")), cex.main=1, display="sites", 
                    cex.lab=0.8, cex.axis=0.8, ylim=c(-0.6, 0.8))
abline(h = 0, lty = "dotted")
abline(v = 0, lty = "dotted")
points(NMDS, "sites", cex=0.8, pch=16, col=year.col[year.group])
ordiellipse(NMDS, groups=year.group, kind="sd", draw="polygon", border=year.col, conf=0.95, alpha=30)
legend("topright", legend=levels(year.group), cex=1, col=year.col, pch=16, pt.cex=1, bty="n")

dev.copy(pdf, "figures/NMDS.year.pdf", height=5.5, width=6)
dev.off() 


######### by sites
ordiplot(NMDS, type="n", main=substitute(paste("")), cex.main=1, display="sites", 
                    cex.lab=0.8, cex.axis=0.8, ylim=c(-0.6, 0.8))
abline(h = 0, lty = "dotted")
abline(v = 0, lty = "dotted")
points(NMDS, "sites", cex=0.8, pch=17, col=col_vector[site.group])
ordiellipse(NMDS, groups=site.group, kind="sd", draw="polygon", 
            border=col_vector, conf=0.95, alpha=30)
legend("topright", legend=levels(site.group), cex=0.5, col=col_vector, pch=17, pt.cex=0.8, bty="n", 
       inset=c(0.4,0.05), x.intersp = 1.2, y.intersp = 0.6)

dev.copy(pdf, "figures/NMDS.site.pdf", height=5.5, width=6)
dev.off()
```

```{r alpha diversity}
sort(rowSums(otu_table(ps.rare))) #reads
rich<-estimate_richness(ps.rare, split = TRUE, measures = NULL)

# plot it
alpha.diversity<-plot_richness(ps.rare, x="year", measures=c("Observed", "Shannon", "Simpson"))

boxplot.richness<- alpha.diversity + 
  geom_boxplot(data = alpha.diversity$data, aes(x = year, y = value, fill=year), alpha = 0.7) +
  scale_fill_manual(values = brewer.pal(4, "BrBG")) +
  theme_classic()   


dev.copy(pdf, "figures/boxplot.alpha.pdf", height=5, width=7)
dev.off() 

```


```{r PCoA and NMDS}
############ PCoA

all_pcoa <- ordinate(
  physeq = ps.rare, 
  method = "PCoA", 
  distance = "bray"
)

PCoA.ord.plot<-plot_ordination(
  physeq = ps.rare,                                                       
  ordination = all_pcoa)+                                                
  geom_point(aes(fill = site, shape = year), size = 3) +  
  stat_ellipse(type = "norm", linetype = 2, aes(color=site)) +
  scale_shape_manual(values = c(21, 22, 23, 24)) +
  scale_fill_manual(values = col_vector) +
  scale_color_manual(values = col_vector, guide="none") +
  theme_classic() +                                                      
  theme(                             
    legend.text = element_text(size = 12),                               
    legend.title = element_blank(),                                      
    legend.background = element_rect(fill = "white", color = "NA"))+  
  theme(axis.text.y.left = element_text(size = 12),
        axis.text.x = element_text(size = 12),
        axis.title.x = element_text(size = 12),
        axis.title.y = element_text(size = 12))+
  guides(fill = guide_legend(override.aes = list(shape = 21))) 

PCoA.ord.plot
dev.copy(pdf, "figures/PCoA.pdf", height=7, width=10)
dev.off() 


############ NMDS
# set k=3 to improve fit with metaMDS

all_NMDS <- ordinate(
  physeq = ps.rare, 
  method = "NMDS", k=3,
  distance = "bray"
)

NMDS.site.year.df<-plot_ordination(
  physeq = ps.rare,                                                       
  ordination = all_NMDS, justDF=TRUE)
# by setting just"df" you get the dataframe that made the plot -- useful if need to hard-code

NMDS.ord.plot<-plot_ordination(
  physeq = ps.rare,                                                       
  ordination = all_NMDS) +                                              
  geom_point(aes(fill = site, shape = year), size = 3) + 
  stat_ellipse(type = "norm", linetype = 2, aes(color=site)) +
  scale_shape_manual(values = c(21, 22, 23, 24)) +
  scale_fill_manual(values = col_vector) +
  scale_color_manual(values = col_vector, guide="none") +
  theme_classic() +                                                       
  theme(                             
    legend.text = element_text(size = 12),                               
    legend.title = element_blank(),                                     
    legend.background = element_rect(fill = "white", color = "NA"))+  
  theme(axis.text.y.left = element_text(size = 12),
        axis.text.x = element_text(size = 12),
        axis.title.x = element_text(size = 12),
        axis.title.y = element_text(size = 12))+
  guides(fill = guide_legend(override.aes = list(shape = 21))) 


NMDS.ord.plot
dev.copy(pdf, "figures/NMDS.pdf", height=7, width=10)
dev.off() 

############ plot by year PCoA and NMDS

PCoA.ord.plot.year<-plot_ordination(
  physeq = ps.rare,                                                   
  ordination = all_pcoa, color="year") +                                                
  geom_point(aes(color = year), size = 3) +    
  stat_ellipse(type = "norm", linetype = 2, aes(color=year)) +
  scale_fill_manual(values = col_vector) +
  scale_color_manual(values = brewer.pal(4, "BrBG")) +
  theme_classic()                                                      
 
PCoA.ord.plot.year
dev.copy(pdf, "figures/PCoA.year.pdf", height=6, width=7)
dev.off() 


NMDS.ord.plot.year<-plot_ordination(
  physeq = ps.rare,                                                   
  ordination = all_NMDS, color="year") +                                                
  geom_point(aes(color = year), size = 3) +    
  stat_ellipse(type = "norm", linetype = 2, aes(color=year)) +
  scale_fill_manual(values = col_vector) +
  scale_color_manual(values = brewer.pal(4, "BrBG")) +
  theme_classic()                                                      
 
NMDS.ord.plot.year
dev.copy(pdf, "figures/NMDS.year.pdf", height=6, width=7)
dev.off() 



```

```{r alpha richness}
# inspect # of reads
sort(rowSums(otu_table(ps.rare))) #reads
rich<-estimate_richness(ps.rare, split = TRUE, measures = NULL)

# plot it
alpha.diversity<-plot_richness(ps.rare, x="year", measures=c("Observed", "Shannon", "Simpson"))

boxplot.richness<- alpha.diversity + 
  geom_boxplot(data = alpha.diversity$data, aes(x = year, y = value, fill=year), alpha = 0.7) +
  scale_fill_manual(values = brewer.pal(4, "BrBG")) +
  theme_classic()   


dev.copy(pdf, "figures/boxplot.alpha.pdf", height=5, width=7)
dev.off() 

```

```{r stacked bar phylum}
# merge all to phylum level

ps.rare_phylum <- ps.rare %>%
  tax_glom(taxrank = "Phylum") %>%                     # agglomerate at phylum level
  transform_sample_counts(function(x) {x/sum(x)} ) %>% # Transform to rel. abundance
  psmelt() %>%                                         # Melt to long format
  filter(Abundance > 0.02) %>%                         # Filter out low abundance taxa
  arrange(Phylum) 

phylum_colors<-col_vector

# Plot 
Bac.Phyla.stacked.bar<-
  ggplot(ps.rare_phylum, aes(x = year, y = Abundance, fill = Phylum)) + 
  #facet_grid(site~.) +
  geom_bar(stat = "identity") +
  scale_fill_manual(values = phylum_colors) +
  #
  guides(fill = guide_legend(reverse = TRUE, keywidth = 1, keyheight = 1)) +
  ylab("Relative Abundance (Phyla > 2%) \n") +
  ggtitle("Phylum Composition of Lake \n Bacterial Communities by Years") 


Bac.Phyla.stacked.bar
dev.copy(pdf, "figures/Bac.Phyla.stacked.bar.pdf", height=5.5, width=6)
dev.off() 

```

```{r hardcode NMDS, eval=FALSE}
#####################
# plotting elements
year.group<-NMDS.ord.plot.year$year
site.group<-NMDS.ord.plot.year$site
# make colors for year
year.col<-brewer.pal(4, "BrBG")

# make colors for sites
library(RColorBrewer)
qual_col_pals = brewer.pal.info[brewer.pal.info$category == 'qual',]
col_vector = unlist(mapply(brewer.pal, qual_col_pals$maxcolors, rownames(qual_col_pals)))

########################

### make plot by habitat with vectors for environment
ordiplot(NMDS, type="n", main=substitute(paste("")), cex.main=1, display="sites", 
                    cex.lab=0.8, cex.axis=0.8, ylim=c(-0.6, 0.8))
abline(h = 0, lty = "dotted")
abline(v = 0, lty = "dotted")
points(NMDS, "sites", cex=0.8, pch=16, col=year.col[year.group])
ordiellipse(NMDS, groups=year.group, kind="sd", draw="polygon", border=year.col, conf=0.95, alpha=30)
legend("topright", legend=levels(year.group), cex=1, col=year.col, pch=16, pt.cex=1, bty="n")

dev.copy(pdf, "figures/NMDS.year.pdf", height=5.5, width=6)
dev.off() 

```

